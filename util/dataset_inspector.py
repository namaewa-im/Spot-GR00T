#!/usr/bin/env python3
"""
í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ ë° íŠ¹ë³„í•œ ë°ì´í„° êµ¬ì¡° ë¶„ì„ ë„êµ¬
ì‚¬ìš©ë²•: python dataset_inspector.py --dataset_path <ë°ì´í„°ì…‹_ê²½ë¡œ> [ì˜µì…˜]
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Dict, Any, List
import pandas as pd
import glob
import numpy as np

try:
    from datasets import load_from_disk, Dataset, DatasetDict
    from datasets.utils.logging import set_verbosity_error
    set_verbosity_error()  # ë¶ˆí•„ìš”í•œ ë¡œê·¸ ìˆ¨ê¸°ê¸°
except ImportError:
    print("ì˜¤ë¥˜: datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. 'pip install datasets'ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    sys.exit(1)


class NumpyEncoder(json.JSONEncoder):
    """numpy ë°°ì—´ê³¼ ê¸°íƒ€ ê°ì²´ë¥¼ JSON ì§ë ¬í™”í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ì¸ì½”ë”"""
    def default(self, obj):
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif hasattr(obj, 'dtype'):
            return str(obj)
        return super().default(obj)


class DatasetInspector:
    def __init__(self, dataset_path: str, verbose: bool = False):
        self.dataset_path = Path(dataset_path)
        self.verbose = verbose
        
    def inspect_dataset(self) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ì„ ë¶„ì„í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜"""
        if not self.dataset_path.exists():
            raise FileNotFoundError(f"ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.dataset_path}")
        
        # ë¨¼ì € ì¼ë°˜ í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ì¸ì§€ í™•ì¸
        try:
            dataset = load_from_disk(str(self.dataset_path))
            return self._analyze_hf_dataset(dataset)
        except Exception as e:
            # ì¼ë°˜ ë°ì´í„°ì…‹ì´ ì•„ë‹ˆë©´ íŠ¹ë³„í•œ êµ¬ì¡°ë¡œ ë¶„ì„
            if self.verbose:
                print(f"lerobot ë°ì´í„°ì…‹ì´ë¯€ë¡œ íŠ¹ë³„í•œ êµ¬ì¡°ë¡œ ë¶„ì„í•©ë‹ˆë‹¤: {e}")
            return self._analyze_special_structure()
    
    def _analyze_hf_dataset(self, dataset) -> Dict[str, Any]:
        """ì¼ë°˜ í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ ë¶„ì„"""
        result = {
            "dataset_path": str(self.dataset_path),
            "dataset_type": "HuggingFace Dataset",
            "splits": {},
            "total_samples": 0,
            "features": {},
            "file_size": self._get_directory_size(),
            "sample_data": {}
        }
        
        if isinstance(dataset, DatasetDict):
            # ì—¬ëŸ¬ ë¶„í• ì´ ìˆëŠ” ê²½ìš°
            for split_name, split_dataset in dataset.items():
                split_info = self._analyze_split(split_dataset, split_name)
                result["splits"][split_name] = split_info
                result["total_samples"] += split_info["num_samples"]
                result["features"][split_name] = split_info["features"]
                result["sample_data"][split_name] = split_info["sample_data"]
        else:
            # ë‹¨ì¼ ë°ì´í„°ì…‹ì¸ ê²½ìš°
            split_info = self._analyze_split(dataset, "main")
            result["splits"]["main"] = split_info
            result["total_samples"] = split_info["num_samples"]
            result["features"]["main"] = split_info["features"]
            result["sample_data"]["main"] = split_info["sample_data"]
        
        return result
    
    def _analyze_special_structure(self) -> Dict[str, Any]:
        """íŠ¹ë³„í•œ ë°ì´í„° êµ¬ì¡° ë¶„ì„ (parquet + ë©”íƒ€ë°ì´í„°)"""
        result = {
            "dataset_path": str(self.dataset_path),
            "dataset_type": "Special Structure (Parquet + Metadata)",
            "file_size": self._get_directory_size(),
            "structure": {},
            "metadata": {},
            "data_files": {},
            "sample_data": {}
        }
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„
        result["structure"] = self._analyze_directory_structure()
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¶„ì„
        result["metadata"] = self._analyze_metadata_files()
        
        # ë°ì´í„° íŒŒì¼ ë¶„ì„
        result["data_files"] = self._analyze_data_files()
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ
        result["sample_data"] = self._extract_sample_data()
        
        return result
    
    def _analyze_directory_structure(self) -> Dict[str, Any]:
        """ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„"""
        structure = {}
        
        for item in self.dataset_path.iterdir():
            if item.is_dir():
                structure[item.name] = {
                    "type": "directory",
                    "contents": [f.name for f in item.iterdir() if f.is_file()]
                }
            else:
                structure[item.name] = {
                    "type": "file",
                    "size": f"{item.stat().st_size / 1024:.1f} KB"
                }
        
        return structure
    
    def _analyze_metadata_files(self) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° íŒŒì¼ ë¶„ì„"""
        metadata = {}
        
        # info.json ë¶„ì„
        info_file = self.dataset_path / "meta" / "info.json"
        if info_file.exists():
            try:
                with open(info_file, 'r', encoding='utf-8') as f:
                    info_data = json.load(f)
                metadata["info.json"] = {
                    "keys": list(info_data.keys()),
                    "content": info_data
                }
            except Exception as e:
                metadata["info.json"] = {"error": str(e)}
        
        # modality.json ë¶„ì„
        modality_file = self.dataset_path / "meta" / "modality.json"
        if modality_file.exists():
            try:
                with open(modality_file, 'r', encoding='utf-8') as f:
                    modality_data = json.load(f)
                metadata["modality.json"] = {
                    "keys": list(modality_data.keys()),
                    "content": modality_data
                }
            except Exception as e:
                metadata["modality.json"] = {"error": str(e)}
        
        # episodes.jsonl ë¶„ì„
        episodes_file = self.dataset_path / "meta" / "episodes.jsonl"
        if episodes_file.exists():
            try:
                episodes = []
                with open(episodes_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        episodes.append(json.loads(line.strip()))
                metadata["episodes.jsonl"] = {
                    "num_episodes": len(episodes),
                    "sample_episodes": episodes[:3] if episodes else []
                }
            except Exception as e:
                metadata["episodes.jsonl"] = {"error": str(e)}
        
        # tasks.jsonl ë¶„ì„
        tasks_file = self.dataset_path / "meta" / "tasks.jsonl"
        if tasks_file.exists():
            try:
                tasks = []
                with open(tasks_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        tasks.append(json.loads(line.strip()))
                metadata["tasks.jsonl"] = {
                    "num_tasks": len(tasks),
                    "sample_tasks": tasks[:3] if tasks else []
                }
            except Exception as e:
                metadata["tasks.jsonl"] = {"error": str(e)}
        
        return metadata
    
    def _analyze_data_files(self) -> Dict[str, Any]:
        """ë°ì´í„° íŒŒì¼ ë¶„ì„"""
        data_files = {}
        
        # parquet íŒŒì¼ ì°¾ê¸°
        parquet_files = list(self.dataset_path.rglob("*.parquet"))
        data_files["parquet_files"] = {
            "count": len(parquet_files),
            "files": []
        }
        
        total_rows = 0
        total_size = 0
        
        for parquet_file in parquet_files[:5]:  # ì²˜ìŒ 5ê°œ íŒŒì¼ë§Œ ë¶„ì„
            try:
                df = pd.read_parquet(parquet_file)
                file_info = {
                    "name": parquet_file.name,
                    "path": str(parquet_file.relative_to(self.dataset_path)),
                    "rows": len(df),
                    "columns": list(df.columns),
                    "size": f"{parquet_file.stat().st_size / 1024:.1f} KB",
                    "dtypes": {col: str(dtype) for col, dtype in df.dtypes.to_dict().items()}
                }
                data_files["parquet_files"]["files"].append(file_info)
                total_rows += len(df)
                total_size += parquet_file.stat().st_size
            except Exception as e:
                data_files["parquet_files"]["files"].append({
                    "name": parquet_file.name,
                    "error": str(e)
                })
        
        data_files["parquet_files"]["total_rows"] = total_rows
        data_files["parquet_files"]["total_size"] = f"{total_size / 1024 / 1024:.1f} MB"
        
        return data_files
    
    def _extract_sample_data(self) -> Dict[str, Any]:
        """ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ"""
        sample_data = {}
        
        # ì²« ë²ˆì§¸ parquet íŒŒì¼ì—ì„œ ìƒ˜í”Œ ì¶”ì¶œ
        parquet_files = list(self.dataset_path.rglob("*.parquet"))
        if parquet_files:
            try:
                df = pd.read_parquet(parquet_files[0])
                sample_data["first_file"] = {
                    "filename": parquet_files[0].name,
                    "sample_rows": df.head(3).to_dict('records')
                }
            except Exception as e:
                sample_data["first_file"] = {"error": str(e)}
        
        return sample_data
    
    def _analyze_split(self, dataset: Dataset, split_name: str) -> Dict[str, Any]:
        """ê°œë³„ ë¶„í•  ë¶„ì„"""
        split_info = {
            "num_samples": len(dataset),
            "features": {},
            "sample_data": {}
        }
        
        # í”¼ì²˜ ì •ë³´ ë¶„ì„
        for feature_name, feature in dataset.features.items():
            feature_info = {
                "dtype": str(feature.dtype),
                "type": type(feature).__name__
            }
            
            # íŠ¹ë³„í•œ í”¼ì²˜ íƒ€ì… ì²˜ë¦¬
            if hasattr(feature, 'num_classes'):
                feature_info["num_classes"] = feature.num_classes
            if hasattr(feature, 'names'):
                feature_info["class_names"] = feature.names
            
            split_info["features"][feature_name] = feature_info
        
        # ìƒ˜í”Œ ë°ì´í„° ì¶”ì¶œ (ì²˜ìŒ 3ê°œ)
        if len(dataset) > 0:
            sample_indices = min(3, len(dataset))
            samples = dataset.select(range(sample_indices))
            
            for i in range(sample_indices):
                sample = samples[i]
                sample_data = {}
                for key, value in sample.items():
                    # ê¸´ í…ìŠ¤íŠ¸ë‚˜ ë¦¬ìŠ¤íŠ¸ëŠ” ì˜ë¼ì„œ í‘œì‹œ
                    if isinstance(value, str) and len(value) > 100:
                        sample_data[key] = value[:100] + "..."
                    elif isinstance(value, list) and len(value) > 10:
                        sample_data[key] = str(value[:10]) + "..."
                    else:
                        sample_data[key] = value
                
                split_info["sample_data"][f"sample_{i+1}"] = sample_data
        
        return split_info
    
    def _get_directory_size(self) -> str:
        """ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°"""
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(self.dataset_path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                total_size += os.path.getsize(filepath)
        
        # í¬ê¸°ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜
        for unit in ['B', 'KB', 'MB', 'GB']:
            if total_size < 1024.0:
                return f"{total_size:.1f} {unit}"
            total_size /= 1024.0
        return f"{total_size:.1f} TB"
    
    def print_summary(self, result: Dict[str, Any]):
        """ìš”ì•½ ì •ë³´ ì¶œë ¥"""
        print("=" * 60)
        print("ğŸ“Š ë°ì´í„°ì…‹ êµ¬ì„± ë¶„ì„ ê²°ê³¼")
        print("=" * 60)
        print(f"ğŸ“ ê²½ë¡œ: {result['dataset_path']}")
        print(f"ğŸ“¦ íƒ€ì…: {result['dataset_type']}")
        print(f"ğŸ“ í¬ê¸°: {result['file_size']}")
        
        if result['dataset_type'] == "HuggingFace Dataset":
            self._print_hf_summary(result)
        else:
            self._print_special_summary(result)
    
    def _print_hf_summary(self, result: Dict[str, Any]):
        """í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ ìš”ì•½ ì¶œë ¥"""
        print(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {result['total_samples']:,}ê°œ")
        print()
        
        # ë¶„í•  ì •ë³´
        print("ğŸ” ë¶„í•  ì •ë³´:")
        for split_name, split_info in result["splits"].items():
            print(f"  â€¢ {split_name}: {split_info['num_samples']:,}ê°œ ìƒ˜í”Œ")
        print()
        
        # í”¼ì²˜ ì •ë³´ (ì²« ë²ˆì§¸ ë¶„í•  ê¸°ì¤€)
        first_split = list(result["splits"].keys())[0]
        print(f"ğŸ“‹ í”¼ì²˜ ì •ë³´ ({first_split}):")
        for feature_name, feature_info in result["features"][first_split].items():
            dtype = feature_info["dtype"]
            feature_type = feature_info["type"]
            print(f"  â€¢ {feature_name}: {dtype} ({feature_type})")
            
            # ì¶”ê°€ ì •ë³´ê°€ ìˆìœ¼ë©´ í‘œì‹œ
            if "num_classes" in feature_info:
                print(f"    - í´ë˜ìŠ¤ ìˆ˜: {feature_info['num_classes']}")
            if "class_names" in feature_info:
                print(f"    - í´ë˜ìŠ¤ëª…: {feature_info['class_names']}")
        print()
        
        # ìƒ˜í”Œ ë°ì´í„°
        print("ğŸ“ ìƒ˜í”Œ ë°ì´í„°:")
        for split_name, sample_data in result["sample_data"].items():
            print(f"  â€¢ {split_name}:")
            for sample_key, sample in sample_data.items():
                print(f"    {sample_key}:")
                for key, value in sample.items():
                    if isinstance(value, str) and len(value) > 50:
                        print(f"      {key}: {value[:50]}...")
                    else:
                        print(f"      {key}: {value}")
                print()
    
    def _print_special_summary(self, result: Dict[str, Any]):
        """íŠ¹ë³„í•œ êµ¬ì¡° ìš”ì•½ ì¶œë ¥"""
        print()
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡°
        print("ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°:")
        for name, info in result["structure"].items():
            if info["type"] == "directory":
                print(f"  â€¢ {name}/ (ë””ë ‰í† ë¦¬)")
                for content in info["contents"][:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                    print(f"    - {content}")
                if len(info["contents"]) > 5:
                    print(f"    ... ì™¸ {len(info['contents']) - 5}ê°œ íŒŒì¼")
            else:
                print(f"  â€¢ {name} ({info['size']})")
        print()
        
        # ë©”íƒ€ë°ì´í„° ì •ë³´
        if result["metadata"]:
            print("ğŸ“‹ ë©”íƒ€ë°ì´í„° ì •ë³´:")
            for meta_name, meta_info in result["metadata"].items():
                if "error" not in meta_info:
                    if "num_episodes" in meta_info:
                        print(f"  â€¢ {meta_name}: {meta_info['num_episodes']}ê°œ ì—í”¼ì†Œë“œ")
                    elif "num_tasks" in meta_info:
                        print(f"  â€¢ {meta_name}: {meta_info['num_tasks']}ê°œ íƒœìŠ¤í¬")
                    elif "keys" in meta_info:
                        print(f"  â€¢ {meta_name}: {len(meta_info['keys'])}ê°œ í‚¤")
                else:
                    print(f"  â€¢ {meta_name}: ì˜¤ë¥˜ - {meta_info['error']}")
            print()
        
        # ë°ì´í„° íŒŒì¼ ì •ë³´
        if "parquet_files" in result["data_files"]:
            parquet_info = result["data_files"]["parquet_files"]
            print("ğŸ“Š ë°ì´í„° íŒŒì¼ ì •ë³´:")
            print(f"  â€¢ Parquet íŒŒì¼ ìˆ˜: {parquet_info['count']}ê°œ")
            print(f"  â€¢ ì´ í–‰ ìˆ˜: {parquet_info['total_rows']:,}ê°œ")
            print(f"  â€¢ ì´ í¬ê¸°: {parquet_info['total_size']}")
            
            if parquet_info["files"]:
                print(f"  â€¢ ì²« ë²ˆì§¸ íŒŒì¼: {parquet_info['files'][0]['name']}")
                print(f"    - í–‰ ìˆ˜: {parquet_info['files'][0]['rows']:,}ê°œ")
                print(f"    - ì»¬ëŸ¼: {', '.join(parquet_info['files'][0]['columns'][:5])}")
                if len(parquet_info['files'][0]['columns']) > 5:
                    print(f"    ... ì™¸ {len(parquet_info['files'][0]['columns']) - 5}ê°œ ì»¬ëŸ¼")
            print()
        
        # ìƒ˜í”Œ ë°ì´í„°
        if result["sample_data"]:
            print("ğŸ“ ìƒ˜í”Œ ë°ì´í„°:")
            for key, data in result["sample_data"].items():
                if "error" not in data:
                    print(f"  â€¢ {key}:")
                    for i, row in enumerate(data["sample_rows"][:2]):  # ì²˜ìŒ 2ê°œ í–‰ë§Œ
                        print(f"    í–‰ {i+1}:")
                        for col, val in list(row.items())[:5]:  # ì²˜ìŒ 5ê°œ ì»¬ëŸ¼ë§Œ
                            if isinstance(val, str) and len(val) > 30:
                                print(f"      {col}: {val[:30]}...")
                            else:
                                print(f"      {col}: {val}")
                        if len(row) > 5:
                            print(f"      ... ì™¸ {len(row) - 5}ê°œ ì»¬ëŸ¼")
                        print()
    
    def print_detailed(self, result: Dict[str, Any]):
        """ìƒì„¸ ì •ë³´ ì¶œë ¥ (JSON í˜•íƒœ)"""
        if self.verbose:
            print("\n" + "=" * 60)
            print("ğŸ” ìƒì„¸ ì •ë³´ (JSON)")
            print("=" * 60)
            print(json.dumps(result, indent=2, ensure_ascii=False, cls=NumpyEncoder))


def main():
    parser = argparse.ArgumentParser(
        description="í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ ë° íŠ¹ë³„í•œ ë°ì´í„° êµ¬ì¡° ë¶„ì„ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python dataset_inspector.py --dataset_path ./data/dataset_name
  python dataset_inspector.py --dataset_path ./data/dataset_name --verbose
  python dataset_inspector.py --dataset_path ./data/dataset_name --json
        """
    )
    
    parser.add_argument(
        "--dataset_path", 
        required=True,
        help="ë¶„ì„í•  ë°ì´í„°ì…‹ ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--verbose", 
        action="store_true",
        help="ìƒì„¸í•œ ì •ë³´ ì¶œë ¥"
    )
    
    parser.add_argument(
        "--json", 
        action="store_true",
        help="JSON í˜•íƒœë¡œë§Œ ì¶œë ¥"
    )
    
    args = parser.parse_args()
    
    try:
        # ë°ì´í„°ì…‹ ë¶„ì„
        inspector = DatasetInspector(args.dataset_path, args.verbose)
        result = inspector.inspect_dataset()
        
        if args.json:
            # JSON í˜•íƒœë¡œë§Œ ì¶œë ¥
            print(json.dumps(result, indent=2, ensure_ascii=False, cls=NumpyEncoder))
        else:
            # ìš”ì•½ ì •ë³´ ì¶œë ¥
            inspector.print_summary(result)
            
            # ìƒì„¸ ì •ë³´ ì¶œë ¥ (verbose ì˜µì…˜ì´ ìˆì„ ë•Œ)
            if args.verbose:
                inspector.print_detailed(result)
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 